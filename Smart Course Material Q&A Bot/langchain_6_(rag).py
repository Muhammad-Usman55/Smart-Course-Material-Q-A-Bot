# -*- coding: utf-8 -*-
"""Langchain 6-(RAG).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PqXNfd8F-snkSQD-VmHihZqREMn5fPLW

# Retrivel Augumentation and Generation

A simple RAG system that:

Loads documents (PDF, text, etc.)

Splits them into chunks

Embeds and stores in a vector DB

Retrieves relevant chunks based on user query

Sends retrieved context + question to Gemini LLM to generate an answer
"""


import os
import getpass

from langchain_community.document_loaders import PyPDFLoader
from langchain.chat_models import init_chat_model
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_google_genai import GoogleGenerativeAIEmbeddings # Corrected import
from langchain.chains import RetrievalQA
from langchain.agents import initialize_agent, AgentType
from langchain_community.utilities import WikipediaAPIWrapper, ArxivAPIWrapper
from langchain_community.tools import WikipediaQueryRun, ArxivQueryRun
from langchain_google_genai import ChatGoogleGenerativeAI
os.environ["GOOGLE_API_KEY"] = getpass.getpass("Enter your Google Gemini API key: ")

# Step 2: Initialize Gemini model
llm = init_chat_model("gemini-2.0-flash", model_provider="google_genai")

# Step 3: Load your document(s) using the correct loader for PDF files
loader = PyPDFLoader("/content/PDC PROJECT.pdf")  # Use PyPDFLoader
documents = loader.load()

# Step 4: Split into chunks
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
docs = text_splitter.split_documents(documents)

# Step 5: Create embeddings using Gemini
embedding_model = GoogleGenerativeAIEmbeddings(model="models/embedding-001")

# Step 6: Store in FAISS vector DB
vectorstore = FAISS.from_documents(docs, embedding=embedding_model)

# Step 7: Create retriever from FAISS
retriever = vectorstore.as_retriever()

# Step 8: Create RAG (RetrievalQA) chain
rag_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,

    return_source_documents=True
)

# Step 9: Ask a question
query = input("‚ùì Ask a question based on the document: ")
result = rag_chain.invoke({"query": query})

# Step 10: Show result and sources
print("\nüìå Answer:")
print(result["result"])

print("\nüìö Source(s):")
for doc in result["source_documents"]:
    # PyPDFLoader adds a 'page' metadata, check for that or fall back
    print("-", doc.metadata.get("source", "[No source metadata]"), f"(page {doc.metadata.get('page', 'N/A')})")

"""## RAG with Agents and Tools"""
# Correct import for chat model

# Step 1: Set Google API key
os.environ["GOOGLE_API_KEY"] = getpass.getpass("Enter your Google Gemini API key: ")

# Step 2: Initialize Gemini model
# Using ChatGoogleGenerativeAI as it's the standard for chat models
llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash", google_api_key=os.environ["GOOGLE_API_KEY"])

# The rest of your code for loading documents, splitting, creating embeddings,
# building FAISS vectorstore, creating retriever, and setting up the RAG chain,
# tools, and agent can follow. Ensure the `docs` variable is loaded before splitting.

# Step 3: Split docs into chunks for vector store indexing
# Assuming 'docs' variable is already loaded from a previous cell execution
# If not, load the documents here:
try:
    docs # Check if docs is defined
except NameError:
    print("Loading documents...")
    from langchain_community.document_loaders import PyPDFLoader
    loader = PyPDFLoader("/content/drive/MyDrive/archive/Pakistan History.pdf")
    docs = loader.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
splitted_docs = text_splitter.split_documents(docs)

# Step 4: Create embeddings model
embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=os.environ["GOOGLE_API_KEY"])

# Step 5: Build FAISS vectorstore with documents
vectorstore = FAISS.from_documents(splitted_docs, embeddings)

# Step 6: Create Retriever
retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k":3})

# Step 7: Build RetrievalQA chain (vanilla)
retrieval_qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)

# --- Now, to add **tools and agent** on top of RAG ---

# Step 8: Create your external tools
wiki_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=300)
wiki_tool = WikipediaQueryRun(api_wrapper=wiki_wrapper)

arxiv_wrapper = ArxivAPIWrapper(top_k_results=1, doc_content_chars_max=300)
arxiv_tool = ArxivQueryRun(api_wrapper=arxiv_wrapper)

tools = [wiki_tool, arxiv_tool]

# Step 9: Initialize agent with tools and LLM
agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True,
)

# Step 10: Define a function combining retrieval and agent use
def rag_agent(query: str):
    # Step 10a: Retrieve relevant documents (context)
    relevant_docs = retriever.get_relevant_documents(query)

    # Combine content from retrieved docs
    context = "\n\n".join([doc.page_content for doc in relevant_docs])

    # Create prompt with context + user query
    prompt = f"Use the following context to answer the question:\n\n{context}\n\nQuestion: {query}"

    # Step 10b: Let the agent answer using tools + context
    answer = agent.run(prompt)

    return answer

# Step 11: Run your RAG + agent pipeline
query = "What were the major challenges faced by Pakistan immediately after independence in 1947, including political leadership, territorial division, and economic conditions? Also tell the recent prime minister of Pakistan."
print(rag_agent(query))
